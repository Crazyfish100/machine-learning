{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d9b940",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Data process\n",
    "from sklearn import feature_selection\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "# Set random seed\n",
    "random.seed(2024)\n",
    "np.random.seed(2024)\n",
    "## loading Data\n",
    "data_file_path =\n",
    "local_data_file_path = \n",
    "dataset=pd.read_csv(data_file_path, encoding='utf-8')\n",
    "dataset_local= pd.read_csv(local_data_file_path, encoding='utf-8')\n",
    "dataset['data_source'] = 'MIMIC'\n",
    "dataset_local['data_source'] = 'Local'\n",
    "dataset_local.head()\n",
    "features = []\n",
    "dataset_1 = dataset[features]\n",
    "dataset_2 = dataset_local[features]\n",
    "\n",
    "data_all = pd.concat([dataset_1, dataset_2], axis=0)\n",
    "data_all\n",
    "## categorical variable\n",
    "data_all.columns = [name.replace('.',' ') for name in data_all.columns]\n",
    "\n",
    "# drop_first=True indicates that the first category is discarded to avoid information redundancy\n",
    "data_all = pd.get_dummies(data_all,drop_first=True)\n",
    "\n",
    "data_all.columns\n",
    "data_all\n",
    "## split,train， test， test_external\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_internal = pd.DataFrame(data_all[data_all[\"data_source_MIMIC\"] == 1])\n",
    "data_external = pd.DataFrame(data_all[data_all[\"data_source_MIMIC\"] == 0])\n",
    "\n",
    "index_train, index_test = train_test_split(range(data_internal.shape[0]), test_size = 0.3)\n",
    "\n",
    "# split dataset\n",
    "data_train = data_internal.loc[index_train].reset_index( drop = True )\n",
    "data_test_internal  = data_internal.loc[index_test].reset_index( drop = True )\n",
    "\n",
    "data_train.drop(columns=['data_source_MIMIC'],inplace=True)\n",
    "data_test_internal.drop(columns=['data_source_MIMIC'],inplace=True)\n",
    "data_external.drop(columns=['data_source_MIMIC'],inplace=True)\n",
    "## Save Data\n",
    "data_train.to_csv('D:/JupyterLab/AKI/Data/data_preprocess/data_train.csv')\n",
    "data_test_internal.to_csv('D:/JupyterLab/AKI/Data/data_preprocess/data_test.csv')\n",
    "data_external.to_csv('D:/JupyterLab/AKI/Data/data_external.csv', encoding='utf-8')\n",
    "## Load Data\n",
    "data_train = pd.read_csv('D:/JupyterLab/AKI/Data/data_preprocess/data_train.csv', encoding='utf-8')\n",
    "data_test = pd.read_csv('D:/JupyterLab/AKI/Data/data_preprocess/data_test.csv', encoding='utf-8')\n",
    "data_external = pd.read_csv('D:/JupyterLab/AKI/Data/data_external.csv', encoding='utf-8')\n",
    "from collections import Counter\n",
    "# def column\n",
    "column_name = ['Age', 'Weight', 'SBP', 'DBP',\n",
    "       'Spo2', 'Glucose', 'SIRS', 'SAPSII', 'SOFA', 'Urine_output', 'RBC',\n",
    "       'WBC', 'Hemoglobin', 'ACEI', 'Beta_blockers', 'ARB', 'Potassium',\n",
    "       'Sodium', 'SCR', 'LDL_C', 'Loop_diuretics', 'Congestive_heart_failure',\n",
    "       'Diabetes', 'Malignancy', 'BUN', 'Hypertension', 'AIP', 'Gender_M']\n",
    "event_column = 'aki_stage'\n",
    "\n",
    "X_train, y_train = data_train[column_name], data_train[event_column]\n",
    "X_test, y_test = data_test[column_name], data_test[event_column]\n",
    "X_test_external, y_test_external = data_external[column_name], data_external[event_column]\n",
    "## Lasso\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "# Define the scope of C\n",
    "C_values = np.logspace(-4, 0, 20)\n",
    "cv_scores_mean = []\n",
    "cv_scores_std = []\n",
    "## Find the optimal C and the C for the 1-SE rule\n",
    "# Calculate cross-validation scores (5-fold, repeated 3 times)\n",
    "for C in C_values:\n",
    "    model = LogisticRegression(\n",
    "        penalty='l1',\n",
    "        solver='saga', \n",
    "        C=C,\n",
    "        max_iter=1000,\n",
    "        random_state=500\n",
    "    )\n",
    "    scores = cross_val_score(\n",
    "        model, X_train, y_train, \n",
    "        cv=10, \n",
    "        scoring='roc_auc' \n",
    "    )\n",
    "    cv_scores_mean.append(np.mean(scores))\n",
    "    cv_scores_std.append(np.std(scores))\n",
    "# Find the optimal C and the C for the 1-SE rule\n",
    "best_C_index = np.argmax(cv_scores_mean)\n",
    "best_C = C_values[best_C_index]\n",
    "one_se_C = C_values[\n",
    "    np.where(cv_scores_mean >= cv_scores_mean[best_C_index] - cv_scores_std[best_C_index])[0][0]\n",
    "]\n",
    "# Print result\n",
    "print(f\"Best C (λ_min): {best_C:.4f}\")\n",
    "print(f\"1-SE rule C (λ_1se): {one_se_C:.4f}\")\n",
    "## Feature selection results\n",
    "# --- Feature selection results ---\n",
    "def get_selected_features(C_value):\n",
    "    model = LogisticRegression(\n",
    "        penalty='l1', solver='saga', \n",
    "        C=C_value, max_iter=10000, \n",
    "        random_state=200\n",
    "    ).fit(X_train, y_train)\n",
    "    selected = X_train.columns[model.coef_[0] != 0].tolist()\n",
    "    return selected\n",
    "# Output the features selected by the two C values\n",
    "print(\"\\n=== Feature selection results ===\")\n",
    "print(f\"Number of features (original): {X_train.shape[1]}\")\n",
    "print(f\"Number of features (C_min): {len(get_selected_features(best_C))}\")\n",
    "print(f\"Number of features (C_1se): {len(get_selected_features(one_se_C))}\")\n",
    "\n",
    "print(\"\\nSelected features (C_min):\", get_selected_features(best_C))\n",
    "print(\"\\nSelected features (C_1se):\", get_selected_features(one_se_C))\n",
    "lasso_features = get_selected_features(best_C)\n",
    "## Boruta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "seed = 42 \n",
    "np.random.seed(seed)\n",
    "#Initialize model\n",
    "rf = RandomForestClassifier(\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced',\n",
    "    max_depth=5,\n",
    "    random_state=seed  \n",
    ")\n",
    "#Boruta parameter settings\n",
    "boruta_params = {\n",
    "    'estimator': rf,\n",
    "    'n_estimators': 'auto',\n",
    "    'verbose': 2,\n",
    "    'random_state': seed,  \n",
    "    'max_iter': 30,      \n",
    "    'perc': 100          \n",
    "}\n",
    "def run_boruta(X, y, n_runs=20):\n",
    "    ranking_df = pd.DataFrame(columns=X.columns)\n",
    "    for i in range(n_runs):\n",
    "        print(f\"\\n=== Run {i+1}/{n_runs} ===\")\n",
    "        boruta_params['random_state'] = seed + i  \n",
    "        selector = BorutaPy(**boruta_params)\n",
    "        selector.fit(X.values, y.values.ravel()) \n",
    "        ranking_df.loc[i] = selector.ranking_\n",
    "    \n",
    "    return ranking_df\n",
    "ranking_df = run_boruta(X_train, y_train, n_runs=20)\n",
    "### Perform feature selection\n",
    "ranking_df.to_csv('boruta_rankings.csv')  # Save the ranking results\n",
    "\n",
    "# Analysis Results\n",
    "# Calculate median rank\n",
    "median_ranks = ranking_df.median().sort_values()\n",
    "\n",
    "# Define selection criteria (indicating selection if median ≤ 2)\n",
    "selected_features = median_ranks[median_ranks <= 2].index.tolist()\n",
    "print(\"\\nUltimately selected features:\", selected_features)\n",
    "boruta_features=selected_features \n",
    "## visualization\n",
    "## Lasso coefficient path plot\n",
    "from sklearn.linear_model import lasso_path\n",
    "eps = 1e-6 \n",
    "alphas, coefs, _ = lasso_path(\n",
    "    X_train.values, y_train.values,\n",
    "    eps=eps,        \n",
    "    n_alphas=100,   \n",
    "    max_iter=10000\n",
    ")\n",
    "# Draw path map\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(coefs.shape[0]):\n",
    "    plt.plot(np.log10(alphas), coefs[i, :], \n",
    "             label=X_train.columns[i], linewidth=1.5)\n",
    "plt.axvline(np.log10(1 / best_C), color='black', linestyle='--', label=r'$C_{min}$')\n",
    "plt.axvline(np.log10(1 / one_se_C), color='blue', linestyle='--', label=r'$C_{1se}$')\n",
    "x_min = min(np.log10(alphas).min(), np.log10(1 / best_C), np.log10(1 / one_se_C))\n",
    "x_max = np.log10(alphas).max()\n",
    "plt.xlim([x_min, x_max])\n",
    "plt.xlabel('log10(lambda)') \n",
    "plt.ylabel('Coefficient value')\n",
    "plt.title('Lasso Coefficient Paths')\n",
    "plt.legend(loc='lower center',  bbox_to_anchor=(0.50, -0.40), ncol=len(X_train.columns)//2, fancybox=True, shadow=True)\n",
    "plt.grid(True)\n",
    "plt.subplots_adjust(left=0.1, right=0.9, bottom=0.3, top=0.9)  \n",
    "plt.savefig('D:/JupyterLab/AKI/Result/Fig/lasso_Coeffcient_Paths2.0.pdf', dpi=800, bbox_inches='tight')\n",
    "plt.show()\n",
    "## Lasso cross-validation curve\n",
    "plt.figure(figsize=(15, 6)) \n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, len(C_values)))\n",
    "\n",
    "for i, (mean_score, std_score, C) in enumerate(zip(cv_scores_mean, cv_scores_std, C_values)):\n",
    "    if i < len(X_train.columns):\n",
    "        feature_name = X_train.columns[i]\n",
    "    else:\n",
    "        break\n",
    "    plt.errorbar(np.log10(1 / C), mean_score, yerr=std_score,\n",
    "                 fmt='o', color=colors[i], ecolor='lightgray', capsize=6,\n",
    "                 label=feature_name)\n",
    "plt.xlabel('log10(lambda)') \n",
    "plt.ylabel('Partial Likelihood Deviance')  \n",
    "plt.title('Lasso Logistic Regression: CV Score vs. log10(lambda)')\n",
    "plt.legend(loc='lower center', bbox_to_anchor=(0.40, -0.32), ncol=max(len(C_values) // 5, 1), fancybox=True, shadow=True)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.savefig('D:/JupyterLab/AKI/Result/Fig/lasso_Selection_no2.0.pdf', dpi=800, bbox_inches='tight')\n",
    "plt.show()\n",
    "### Boruta boxplot\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(data=ranking_df[median_ranks.index], palette=\"Blues\")\n",
    "plt.axhline(y=2.5, color='r', linestyle='--', label='Selection Threshold')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(f\"Boruta Feature Selection\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('D:/JupyterLab/AKI/Result/Fig/boruta_results2.0.pdf', dpi=800, bbox_inches='tight')\n",
    "plt.show()\n",
    "### Intersection network diagram\n",
    "import networkx as nx\n",
    "lasso_features = get_selected_features(best_C)\n",
    "\n",
    "if lasso_features is None:\n",
    "    print((\"lasso_features is None. Please check the feature selection process.\"). \")\n",
    "else:\n",
    "    # Create a set for finding the intersection\n",
    "    boruta_set = set(boruta_features)  # Boruta feature set\n",
    "    lasso_set = set(lasso_features)    # Lasso feature set\n",
    "    intersection = boruta_set.intersection(lasso_set)  # intersection of two sets\n",
    "    boruta_only = boruta_set - intersection \n",
    "    lasso_only = lasso_set - intersection    # Features selected only by Lasso\n",
    "\n",
    "    print(\"Features selected by both methods:\", intersection)\n",
    "    print(\"Only features selected by Boruta:\", boruta_only)\n",
    "    print(\"Only features selected by Lasso:\", lasso_only)\n",
    "G = nx.Graph()\n",
    "for feature in boruta_only:\n",
    "    G.add_edge('Boruta', feature, color='lightcoral')  \n",
    "    G.add_edge('Lasso', feature, color='lightblue')  \n",
    "for feature in intersection:\n",
    "    G.add_edge('Boruta', feature, color='lightcoral')  \n",
    "    G.add_edge('Lasso', feature, color='lightblue')\n",
    "edge_colors = [data['color'] for _, _, data in G.edges(data=True)]\n",
    "node_colors = []\n",
    "for node in G.nodes():\n",
    "    if node == 'Boruta':\n",
    "        node_colors.append('lightcoral') \n",
    "    elif node == 'Lasso':\n",
    "        node_colors.append('lightblue')  \n",
    "    elif node in boruta_only:\n",
    "        node_colors.append('lightcoral')  \n",
    "    elif node in lasso_only:\n",
    "        node_colors.append('lightblue')  \n",
    "        node_colors.append('plum')     \n",
    "plt.figure(figsize=(10, 10))\n",
    "pos = nx.spring_layout(G, seed=42) \n",
    "nx.draw_networkx(\n",
    "    G,\n",
    "    pos,\n",
    "    edge_color=edge_colors,\n",
    "    node_color=node_colors,\n",
    "    with_labels=True,\n",
    "    node_size=1000,\n",
    "    font_size=10,\n",
    "    edgecolors='none' \n",
    ")\n",
    "plt.title('Feature Selection by Boruta and Lasso')\n",
    "\n",
    "plt.savefig('D:/JupyterLab/AKI/Result/Fig/feature_selection_boruta_lasso5.pdf', format='pdf', dpi=300)\n",
    "pd.Series(list(intersection)).to_csv('D:\\\\JupyterLab\\\\AKI\\\\Result\\\\Fig\\\\selected_features_intersection.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5505995b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Data process\n",
    "from sklearn import feature_selection\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "## Load Data\n",
    "data_train = pd.read_csv('D:/JupyterLab/AKI/Data/data_preprocess/data_train.csv', encoding='utf-8')\n",
    "data_test = pd.read_csv('D:/JupyterLab/AKI/Data/data_preprocess/data_test.csv', encoding='utf-8')\n",
    "data_external = pd.read_csv('D:/JupyterLab/AKI/Data/data_external.csv', encoding='utf-8')\n",
    "data_train.columns\n",
    "### 用筛选的变量\n",
    "from collections import Counter\n",
    "# def column\n",
    "column_name = ['Age', 'Weight', 'Glucose', 'SBP', 'DBP',\n",
    "               'SpO2', 'SCR', 'Urineoutput', 'SAPSII', 'SIRS', 'SOFA',  'ARNI',\n",
    "               'Loop diuretics','Congestiveheartfailure', 'WBC', 'BUN',  'AIP', ]\n",
    "event_column = 'aki_stage'\n",
    "\n",
    "X_train, y_train = data_train[column_name], data_train[event_column]\n",
    "X_test, y_test = data_test[column_name], data_test[event_column]\n",
    "X_test_external, y_test_external = data_external[column_name], data_external[event_column]\n",
    "# Model Turning\n",
    "## 1. XGboost\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import  RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "### XGboost Turning (random search)\n",
    "# 定义参数空间，加入正则化参数\n",
    "random_param_dist = {\n",
    "    'learning_rate': np.linspace(0.01, 0.3, 10),\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'max_depth': [3, 4, 5, 6, 7],\n",
    "    'min_child_weight': [1, 2, 3, 4],\n",
    "    'subsample': np.linspace(0.5, 1, 5),\n",
    "    'colsample_bytree': np.linspace(0.5, 1, 5),\n",
    "    'reg_alpha': np.linspace(0, 1, 5),  \n",
    "    'reg_lambda': np.linspace(1, 3, 5)  \n",
    "}\n",
    "# 初始化XGBoost分类器\n",
    "xgb_classifier = xgb.XGBClassifier(objective='binary:logistic')\n",
    "cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "## 调参\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_classifier,\n",
    "    param_distributions=random_param_dist,\n",
    "    n_iter=50,\n",
    "    cv=cv,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=4,\n",
    "    random_state=100\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "# 打印随机搜索的最佳参数\n",
    "print(random_search.best_params_)\n",
    "#Obtain the best model\n",
    "random_search.fit(X_train, y_train)\n",
    "best_xgb_model = random_search.best_estimator_\n",
    "#Print the optimal parameters for random search\n",
    "Print (\"Random search for best parameters:\", random_dearch. best_rams_)\n",
    "print(\"Best Score : \", random_search.score(X_test, y_test))\n",
    "### Save XGBoost Model\n",
    "from joblib import dump, load\n",
    "model_path = r'D:\\JupyterLab\\AKI\\Model\\XGBoost_model4.joblib'\n",
    "dump(best_xgb_model, model_path)\n",
    "print(f\"The model has been saved to {model_path}\")\n",
    "## 2. SVM Model\n",
    "from itertools import cycle\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.metrics import cohen_kappa_score, roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "#### K折交叉验证\n",
    "#### 线性核函数算法\n",
    "param_grid_linear = {'C': [0.1, 1, 10]}  \n",
    "cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "model_linear = GridSearchCV(SVC(kernel=\"linear\", random_state=100), param_grid_linear, cv=cv)\n",
    "model_linear.fit(X_train, y_train)\n",
    "print(\"Best Parameters for Linear Kernel: \", model_linear.best_params_)\n",
    "best_model_linear = model_linear.best_estimator_\n",
    "print(\"Best Score for Linear Kernel: \", best_model_linear.score(X_test, y_test))\n",
    "from joblib import dump, load\n",
    "# 保存模型\n",
    "model_path = r'D:\\JupyterLab\\AKI\\Model\\SVM_model_linear4.joblib'\n",
    "dump(best_model_linear, model_path)\n",
    "\n",
    "#### 多项式核函数算法\n",
    "param_grid_poly = {'C': [0.1, 1, 10], 'gamma': [0.01, 0.1, 1]}\n",
    "cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "model_poly = GridSearchCV(SVC(kernel=\"poly\", random_state=123), param_grid_poly, cv=cv)\n",
    "model_poly.fit(X_train, y_train)\n",
    "print(\"Best Parameters for Polynomial Kernel: \", model_poly.best_params_)\n",
    "best_model_poly = model_poly.best_estimator_\n",
    "print(\"Best Score for Polynomial Kernel: \", best_model_poly.score(X_test, y_test))\n",
    "print(\"Number of Support Vectors for Polynomial Kernel: \", len(best_model_poly.support_))\n",
    "from joblib import dump, load\n",
    "# 保存模型\n",
    "model_path = r'D:\\JupyterLab\\AKI\\Model\\SVM_model_poly4.joblib'\n",
    "dump(best_model_poly, model_path)\n",
    "\n",
    "#### 径向基函数算法\n",
    "param_grid_poly = {'C': [0.1, 1, 10], 'gamma': [0.01, 0.1, 1]}\n",
    "cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "model_rbf = GridSearchCV(SVC(kernel=\"rbf\", random_state=200), param_grid_poly, cv=cv)  \n",
    "print(\"Best Parameters for RBF Kernel: \", model_rbf.best_params_)\n",
    "best_model_rbf = model_rbf.best_estimator_\n",
    "print(\"Best Score for RBF Kernel: \", best_model_rbf.score(X_test, y_test))\n",
    "print(\"Number of Support Vectors for RBF Kernel: \", len(best_model_rbf.support_))\n",
    "from joblib import dump, load\n",
    "\n",
    "model_path = r'D:\\JupyterLab\\AKI\\Model\\SVM_model_rbf4.joblib'\n",
    "dump(best_model_rbf, model_path)\n",
    "\n",
    "#### sigmoid核函数算法\n",
    "param_grid_poly = {'C': [0.1, 1, 10], 'gamma': [0.01, 0.1, 1]}\n",
    "cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "model_sigmoid = GridSearchCV(SVC(kernel=\"sigmoid\", random_state=100), param_grid_poly, cv=cv)  \n",
    "model_sigmoid.fit(X_train, y_train)\n",
    "print(\"Best Parameters for Sigmoid Kernel: \", model_sigmoid.best_params_)\n",
    "best_model_sigmoid = model_sigmoid.best_estimator_\n",
    "print(\"Best Score for Sigmoid Kernel: \", best_model_sigmoid.score(X_test, y_test))\n",
    "print(\"Number of Support Vectors for Sigmoid Kernel: \", len(best_model_sigmoid.support_))\n",
    "from joblib import dump, load\n",
    "# 保存模型\n",
    "model_path = r'D:\\JupyterLab\\AKI\\Model\\SVM_model_sigmoid4.joblib'\n",
    "dump(best_model_sigmoid, model_path)\n",
    "\n",
    "## 3. Logistic\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from scipy.stats import loguniform  \n",
    "param_dist = {\n",
    "    'C': loguniform(1e-4, 100), \n",
    "    'penalty': ['l1', 'l2'],    \n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'max_iter': [100, 500, 1000], \n",
    "    'class_weight': [None, 'balanced']  \n",
    "}\n",
    "cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "### 随机搜索\n",
    "model = LogisticRegression()\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50, \n",
    "    cv=cv ,       \n",
    "    scoring='accuracy',\n",
    "    random_state=200,\n",
    "    n_jobs=4    \n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "### 训练逻辑回归模型\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Best CV Score:\", random_search.best_score_) \n",
    "print(\"Best Params:\", random_search.best_params_) \n",
    "\n",
    "\n",
    "model_logistic = random_search.best_estimator_\n",
    "test_score = model_logistic.score(X_test, y_test)\n",
    "print(\"True Test Score:\", test_score)  \n",
    "### Save Logistict model\n",
    "from joblib import dump, load\n",
    "# 保存模型\n",
    "model_path = r'D:\\JupyterLab\\AKI\\Model\\Logistic_model4.joblib'\n",
    "dump(model_logistic, model_path)\n",
    "\n",
    "## 4. Elastic Net Regression\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "### 随机搜索调参\n",
    "# 创建分层K折交叉验证对象（保持类别分布）\n",
    "cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=100)\n",
    "# 创建弹性网络逻辑回归模型\n",
    "logistic_elastic_net = LogisticRegression(\n",
    "    penalty='elasticnet',\n",
    "    solver='saga',       \n",
    "    random_state=42,\n",
    "    max_iter=1000\n",
    ")  \n",
    "# 定义参数网格\n",
    "param_grid = {\n",
    "    'solver': ['saga'],\n",
    "    'penalty': ['elasticnet'],\n",
    "    'l1_ratio': uniform(0, 1), \n",
    "    'C': uniform(0.01, 10),     \n",
    "    'class_weight': [None, 'balanced']  \n",
    "}\n",
    "# 创建随机搜索对象\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=logistic_elastic_net,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50,            \n",
    "    cv=cv,  # 使用10折\n",
    "    scoring='roc_auc',    \n",
    "    n_jobs=4,           \n",
    "    random_state=100,\n",
    "    verbose=2,\n",
    "    refit=True           \n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "# 输出结果\n",
    "print(\"最佳参数:\", random_search.best_params_)\n",
    "print(\"交叉验证AUC:\", random_search.best_score_)\n",
    "print(\"测试集AUC:\", random_search.score(X_test, y_test))\n",
    "# 使用随机搜索最佳参数创建模型\n",
    "best_en_model = random_search.best_estimator_\n",
    "### Save Elastic Net Regression model\n",
    "from joblib import dump, load\n",
    "model_path = r'D:\\JupyterLab\\AKI\\Model\\Elastic_Net_Regression_model4.joblib'\n",
    "dump(best_en_model, model_path)\n",
    "\n",
    "## 5.AdaBoost\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import plot_roc_curve \n",
    "# 初始化AdaBoost分类器\n",
    "model_AdaBoostClassifier = AdaBoostClassifier(random_state=100)\n",
    "# 训练模型\n",
    "model_AdaBoostClassifier.fit(X_train, y_train)\n",
    "# 在测试集上评估模型\n",
    "score = model_AdaBoostClassifier.score(X_test, y_test)\n",
    "print(\"AdaBoost算法得分：\", score)\n",
    "### 随机搜索调参\n",
    "#定义参数\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300], \n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0], \n",
    "    'algorithm': ['SAMME', 'SAMME.R'] }\n",
    "# 设定交叉验证方法\n",
    "cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=100)\n",
    "# 初始化随机搜索模型\n",
    "random_search = RandomizedSearchCV(\n",
    "    AdaBoostClassifier(random_state=123),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    cv=cv,\n",
    "    random_state=42\n",
    ")\n",
    "# 在训练集上做K折调参（不碰测试集）\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best CV Score:\", random_search.best_score_) \n",
    "print(\"Best Params:\", random_search.best_params_)  \n",
    "\n",
    "best_adb_model = random_search.best_estimator_\n",
    "test_score = best_adb_model.score(X_test, y_test)\n",
    "print(\"True Test Score:\", test_score)  \n",
    "### Save Adaboost model\n",
    "from joblib import dump, load\n",
    "\n",
    "# 保存模型\n",
    "model_path = r'D:\\JupyterLab\\AKI\\Model\\AdaBoost_model4.joblib'\n",
    "dump(best_adb_model, model_path)\n",
    "\n",
    "## 6.Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "# 创建一个随机森林分类器\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=123)\n",
    "### 随机搜索\n",
    "# 创建 RandomizedSearchCV 对象\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50, \n",
    "    cv=cv, \n",
    "    scoring='roc_auc', \n",
    "    random_state=100,\n",
    "    n_jobs=4 \n",
    ")\n",
    "\n",
    "# 进行随机搜索\n",
    "random_search.fit(X_train, y_train)\n",
    "#Obtain the optimal parameters\n",
    "best_params = random_search.best_params_\n",
    "#Print optimal parameters\n",
    "print(\"Best Parameters: \", random_search.best_params_)\n",
    "#Print the score of the best estimator on the test set\n",
    "print(\"Best Score: \", random_search.score(X_test, y_test))\n",
    "\n",
    "best_rf = RandomForestClassifier(**best_params, random_state=42)\n",
    "best_rf.fit(X_train, y_train)\n",
    "best_rf_model = best_rf\n",
    "### Save Random Forest model\n",
    "from joblib import dump, load\n",
    "\n",
    "# 保存模型\n",
    "model_path = r'D:\\JupyterLab\\AKI\\Model\\RF_model4.joblib'\n",
    "dump(best_rf_model, model_path)\n",
    "print(f\"模型已保存到 {model_path}\")\n",
    "## 7.Ridge\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from scipy.stats import loguniform\n",
    "#Classification model optimization\n",
    "ridge_clf = RidgeClassifier(random_state=42)  \n",
    "\n",
    "#Define parameter distribution\n",
    "param_dist = {\n",
    "'alpha': loguniform(1e-4, 1e4),\n",
    "'class_weight': [None, 'balanced'],\n",
    "'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg']\n",
    "}\n",
    "#Cross validation strategy\n",
    "cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=100)\n",
    "### 随机搜索\n",
    "# 随机搜索\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=ridge_clf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    cv=cv,\n",
    "    scoring='roc_auc',\n",
    "    random_state=100,\n",
    "    n_jobs=4,\n",
    "    verbose=1\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best CV Score:\", random_search.best_score_)  \n",
    "print(\"Best Params:\", random_search.best_params_)   \n",
    "\n",
    "best_clf_model = random_search.best_estimator_\n",
    "test_score = best_clf_model.score(X_test, y_test)\n",
    "print(\"True Test Score:\", test_score) \n",
    "### Save Ridge moel\n",
    "from joblib import dump, load\n",
    "\n",
    "# 保存模型\n",
    "model_path = r'D:\\JupyterLab\\AKI\\Model\\Ridge_model4.joblib'\n",
    "dump(best_clf_model, model_path)\n",
    "\n",
    "## 8.KNN\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score, RocCurveDisplay, \n",
    "                             confusion_matrix, classification_report)\n",
    "from scipy.stats import randint\n",
    "\n",
    "# 交叉验证策略\n",
    "cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=100)\n",
    "\n",
    "param_dist = {\n",
    "    'n_neighbors': randint(1, 100),               \n",
    "    'weights': ['uniform', 'distance'],         \n",
    "    'p': [1, 1.5, 2, 3],                         \n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree'],\n",
    "    'leaf_size': randint(10, 100)        \n",
    "}\n",
    "\n",
    "# 初始化KNN模型\n",
    "knn = KNeighborsClassifier()\n",
    "### 随机搜索\n",
    "#配置随机搜索（集成K折交叉验证）\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=knn,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,                 \n",
    "    cv=cv,        \n",
    "    scoring='roc_auc',\n",
    "    n_jobs=4,                  \n",
    "    random_state=100,\n",
    "    verbose=2,\n",
    "    refit=True                  \n",
    ")\n",
    "# 执行搜索\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best CV Score:\", random_search.best_score_)  \n",
    "print(\"Best Params:\", random_search.best_params_)  \n",
    "\n",
    "best_knn_model  = random_search.best_estimator_\n",
    "test_score = best_knn_model .score(X_test, y_test)\n",
    "print(\"True Test Score:\", test_score) \n",
    "### Save KNN model\n",
    "from joblib import dump, load\n",
    "\n",
    "# 保存模型\n",
    "model_path = r'D:\\JupyterLab\\AKI\\Model\\KNN_model4.joblib'\n",
    "dump(best_knn_model, model_path)\n",
    "\n",
    "## 9.SGD\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from scipy.stats import loguniform, uniform\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, log_loss\n",
    "from copy import deepcopy\n",
    "#初始化基础模型\n",
    "base_model = SGDClassifier(\n",
    "    loss='log',  \n",
    "    penalty='elasticnet',\n",
    "    random_state=42,\n",
    "    max_iter=10000, \n",
    "    warm_start=False,  \n",
    "    learning_rate='optimal' )\n",
    "#定义参数分布（关键参数搜索空间）\n",
    "param_dist = {\n",
    "    'alpha': loguniform(1e-6, 1e-2),\n",
    "    'l1_ratio': uniform(0, 1),\n",
    "    'eta0': loguniform(1e-4, 1e-1), \n",
    "    'power_t': uniform(0.1, 0.5) \n",
    "}\n",
    "\n",
    "# 交叉验证策略\n",
    "cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=100)\n",
    "### 随机搜索\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    cv=cv,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=3,  \n",
    "    random_state=30,\n",
    "    verbose=2,\n",
    "    error_score='raise' \n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best CV Score:\", random_search.best_score_) \n",
    "print(\"Best Params:\", random_search.best_params_)   \n",
    "\n",
    "best_sgd_model  = random_search.best_estimator_\n",
    "test_score = best_sgd_model .score(X_test, y_test)\n",
    "print(\"True Test Score:\", test_score) \n",
    "### Save SGD model\n",
    "from joblib import dump, load\n",
    "\n",
    "# 保存模型\n",
    "model_path = r'D:\\JupyterLab\\AKI\\Model\\SGD_model4.joblib'\n",
    "dump(best_sgd_model, model_path)\n",
    "\n",
    "## 10.ANN\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "param_dist = {\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['sgd'],\n",
    "    'alpha': [0.0001, 0.001],\n",
    "    'learning_rate': ['constant'], \n",
    "    'learning_rate_init': [0.001, 0.01, 0.1], \n",
    "    'max_iter': [200, 300, 1000],\n",
    "    'hidden_layer_sizes': [(128,), (64,), (128, 64), (64, 32), (128, 64, 32)],\n",
    "    'batch_size': [32, 64, 128]\n",
    "}\n",
    "\n",
    "ann = MLPClassifier(random_state=42)\n",
    "# 交叉验证策略\n",
    "cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=100)\n",
    "### 随机搜索\n",
    "# 创建随机搜索对象\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=ann,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    cv=cv,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=4,\n",
    "    verbose=2,\n",
    "    random_state=60\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best CV Score:\", random_search.best_score_)  \n",
    "print(\"Best Params:\", random_search.best_params_)  \n",
    "\n",
    "\n",
    "best_ann_model  = random_search.best_estimator_\n",
    "test_score = best_ann_model .score(X_test, y_test)\n",
    "print(\"True Test Score:\", test_score) \n",
    "### Save ANN model\n",
    "from joblib import dump, load\n",
    "\n",
    "# 保存模型\n",
    "model_path = r'D:\\JupyterLab\\AKI\\Model\\ANN_model4.joblib'\n",
    "dump(best_ann_model, model_path)\n",
    "\n",
    "## 11.Bagging\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import (accuracy_score, roc_auc_score, \n",
    "                             confusion_matrix, classification_report)\n",
    "from scipy.stats import uniform, randint\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "\n",
    "# 交叉验证策略\n",
    "cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=100)\n",
    "# --------------------- 基础XGBoost模型 ---------------------\n",
    "base_xgb = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    booster='gbtree',\n",
    "    eval_metric='auc',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --------------------- 超参数搜索空间 ---------------------\n",
    "param_dist = {\n",
    "    'base_estimator__max_depth': randint(3, 10),\n",
    "    'base_estimator__learning_rate': uniform(0.01, 0.3),\n",
    "    'base_estimator__n_estimators': randint(50, 300),\n",
    "    'base_estimator__subsample': uniform(0.6, 0.4),\n",
    "    'base_estimator__colsample_bytree': uniform(0.6, 0.4),\n",
    "    'base_estimator__gamma': uniform(0, 2),\n",
    "    'n_estimators': randint(5, 20), \n",
    "    'max_samples': uniform(0.6, 0.4)\n",
    "}\n",
    "# --------------------- 创建Bagging集成 ---------------------\n",
    "bagging_xgb = BaggingClassifier(\n",
    "    base_estimator=base_xgb,\n",
    "    bootstrap=True,\n",
    "    random_state=100,\n",
    "    n_jobs=-1\n",
    ")\n",
    "### 随机搜索\n",
    "# --------------------- 随机搜索优化 ---------------------\n",
    "random_search= RandomizedSearchCV(\n",
    "    estimator=bagging_xgb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,  # 搜索迭代次数\n",
    "    cv=cv,       # 交叉验证折数\n",
    "    scoring='roc_auc',\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"Best CV Score:\", random_search.best_score_) \n",
    "print(\"Best Params:\", random_search.best_params_)  \n",
    "\n",
    "\n",
    "# 用最优模型独立评估测试集\n",
    "best_Bagging_xgb = random_search.best_estimator_\n",
    "test_score = best_Bagging_xgb .score(X_test, y_test)\n",
    "print(\"True Test Score:\", test_score)  \n",
    "### Save Bagging model\n",
    "from joblib import dump, load\n",
    "best_Bagging_model=best_Bagging_xgb\n",
    "# 保存模型\n",
    "model_path = r'D:\\JupyterLab\\AKI\\Model\\Bagging_model4.joblib'\n",
    "dump(best_Bagging_model, model_path)\n",
    "\n",
    "## 12. MLP\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from scipy.stats import loguniform, randint\n",
    "mlp = MLPClassifier(max_iter=500, random_state=42)  \n",
    "\n",
    "param_dist = {\n",
    "    'hidden_layer_sizes': [(50,), (30, 20)],  \n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'alpha': loguniform(1e-5, 1e-2),  \n",
    "    'learning_rate_init': [0.001, 0.01]  \n",
    "}\n",
    "\n",
    "# 交叉验证策略\n",
    "cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=100)\n",
    "### 随机搜索\n",
    "# 随机搜索调参\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=mlp,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,                     \n",
    "    cv=cv,\n",
    "    scoring='roc_auc',             \n",
    "    n_jobs=4,                     \n",
    "    random_state=100,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best CV Score:\", random_search.best_score_)  \n",
    "print(\"Best Params:\", random_search.best_params_)  \n",
    "\n",
    "best_MLP_model  = random_search.best_estimator_\n",
    "test_score = best_MLP_model .score(X_test, y_test)\n",
    "print(\"True Test Score:\", test_score)\n",
    "### Save MLP model\n",
    "from joblib import dump, load\n",
    "\n",
    "# 保存模型\n",
    "model_path = r'D:\\JupyterLab\\AKI\\Model\\MLP_model4.joblib'\n",
    "dump(best_MLP_model, model_path)\n",
    "\n",
    "## 13. Catboost\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    " random_state = 42\n",
    "# 定义超参数分布\n",
    "param_dist_cat = {\n",
    "    'iterations': [500, 1000, 1500],  \n",
    "    'learning_rate': [0.01, 0.05, 0.1], \n",
    "    'depth': [4, 6, 8],              \n",
    "    'l2_leaf_reg': [1, 3, 5],        \n",
    "    'eval_metric': ['F1'],           \n",
    "    'random_seed': [random_state],\n",
    "    'verbose': [0]\n",
    "   \n",
    "}\n",
    "# 定义 K 折交叉验证\n",
    "\n",
    "cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=100)\n",
    "\n",
    "catboost = CatBoostClassifier()\n",
    "### 随机搜索\n",
    "# 配置随机搜索\n",
    "random_search_cat = RandomizedSearchCV(\n",
    "    estimator=catboost,\n",
    "    param_distributions=param_dist_cat,\n",
    "    n_iter=50,                     \n",
    "    scoring='roc_auc',              \n",
    "    cv=cv,                         \n",
    "    n_jobs=4,                      \n",
    "    verbose=2,\n",
    "    random_state=100,\n",
    "    refit=True   )                   \n",
    "# 执行随机搜索\n",
    "random_search_cat.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=(X_test, y_test),      \n",
    "    early_stopping_rounds=20         \n",
    ")\n",
    "\n",
    "print(\"Best CV Score:\", random_search_cat.best_score_) \n",
    "print(\"Best Params:\", random_search_cat.best_params_)  \n",
    "\n",
    "\n",
    "best_catboost_model  = random_search_cat.best_estimator_\n",
    "test_score = best_catboost_model .score(X_test, y_test)\n",
    "print(\"True Test Score:\", test_score)  \n",
    "# 评估测试集\n",
    "best_cat = random_search_cat.best_estimator_\n",
    "y_pred_cat = best_cat.predict(X_test)\n",
    "y_proba_cat = best_cat.predict_proba(X_test)[:, 1]  \n",
    "print(f\"\\n测试集性能：\")\n",
    "print(f\"准确率：{accuracy_score(y_test, y_pred_cat):.4f}\")\n",
    "print(f\"F1-score：{f1_score(y_test, y_pred_cat):.4f}\")\n",
    "print(f\"AUC-ROC：{roc_auc_score(y_test, y_proba_cat):.4f}\")\n",
    "### Save Catboost model\n",
    "from joblib import dump, load\n",
    "\n",
    "# 保存模型\n",
    "model_path = r'D:\\JupyterLab\\AKI\\Model\\Catboost_model4.joblib'\n",
    "dump(best_catboost_model, model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e79a35d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "import tensorflow as tf\n",
    "from joblib import dump, load\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Load MLP Model\n",
    "MLP_model = load(r'D:\\JupyterLab\\AKI\\Model\\MLP_model4.joblib')\n",
    "#\n",
    "## Load ADB Model\n",
    "ADB_model = load( r'D:\\JupyterLab\\AKI\\Model\\AdaBoost_model4.joblib')\n",
    "#\n",
    "## Load ANN Model\n",
    "ANN_model = load(r'D:\\JupyterLab\\AKI\\Model\\ANN_model4.joblib')\n",
    "#\n",
    "## Load Bagging Model\n",
    "Bagging_model = load(r'D:\\JupyterLab\\AKI\\Model\\Bagging_model4.joblib')\n",
    "#\n",
    "## Load en Model\n",
    "Elastic_Net_Regression_model= load(r'D:\\JupyterLab\\AKI\\Model\\Elastic_Net_Regression_model4.joblib')\n",
    "#\n",
    "## Load knn Model\n",
    "KNN_model= load( r'D:\\JupyterLab\\AKI\\Model\\KNN_model4.joblib')\n",
    "#\n",
    "## Load Logistic Model\n",
    "Logistic_model= load(r'D:\\JupyterLab\\AKI\\Model\\Logistic_model4.joblib')\n",
    "#\n",
    "## Load Ridege Model\n",
    "Rideg_model= load(r'D:\\JupyterLab\\AKI\\Model\\Ridge_model4.joblib')\n",
    "#\n",
    "## Load SGD Model\n",
    "SGD_model= load(r'D:\\JupyterLab\\AKI\\Model\\SGD_model4.joblib')\n",
    "#\n",
    "## Load SVM Model\n",
    "SVM_model= load(r'D:\\JupyterLab\\AKI\\Model\\SVM_model_rbf4.joblib')\n",
    "#\n",
    "## Load xgb Model\n",
    "XGB_model= load(r'D:\\JupyterLab\\AKI\\Model\\XGBoost_model5.joblib')\n",
    "#\n",
    "## Load RF Model\n",
    "RF_model= load(r'D:\\JupyterLab\\AKI\\Model\\RF_model4.joblib')\n",
    "#\n",
    "## Load CAB Model\n",
    "Catboost_model= load(r'D:\\JupyterLab\\AKI\\Model\\Catboost_model4.joblib')\n",
    "\n",
    "models = {\n",
    "    \"MLP\": MLP_model,\n",
    "    \"AdaBoost\": ADB_model,\n",
    "    \"Bagging\": Bagging_model,\n",
    "    \"Elastic Net Regression\": Elastic_Net_Regression_model,\n",
    "    \"ANN\": ANN_model,\n",
    "    \"Random Forest\": RF_model,\n",
    "    \"KNN\": KNN_model,\n",
    "    \"Catboost\": Catboost_model,\n",
    "    \"Logistic\": Logistic_model,\n",
    "    \"Ridge\": Rideg_model,\n",
    "    \"SGD\": SGD_model,\n",
    "    \"SVM\": SVM_model,\n",
    "    \"XGBoost\": XGB_model\n",
    "}\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import check_consistent_length\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import (roc_curve, auc, brier_score_loss, \n",
    "                            accuracy_score, recall_score, \n",
    "                            precision_score, f1_score, \n",
    "                            confusion_matrix)\n",
    "### Internal\n",
    "#Generate prediction results\n",
    "predictions = {}\n",
    "\n",
    "for model_key, model_ in models.items():\n",
    "Print (f \"\\ n Processing model: {model_key}\")\n",
    "try:\n",
    "       \n",
    "if hasattr(model_, \"predict_proba\"):\n",
    "predictions[f\"{model_key}\"] = model_.predict_proba(X_test)[:, 1]\n",
    "else:\n",
    "predictions[f\"{model_key}\"]  = model_.decision_function(X_test)\n",
    "\n",
    "  \n",
    "    \n",
    "except Exception as e:\n",
    "Print (f \"Error processing model {model_key}: {str (e)}\")\n",
    "continue    \n",
    "### External\n",
    "#Generate prediction results\n",
    "predictions_external = {}\n",
    "\n",
    "for model_key, model_ in models.items():\n",
    "Print (f \"\\ n Processing model: {model_key}\")\n",
    "try:\n",
    "#Obtain predicted values\n",
    "if hasattr(model_, \"predict_proba\"):\n",
    "predictions_external[f\"{model_key}\"] = model_.predict_proba(X_test_external)[:, 1]\n",
    "else:\n",
    "predictions_external[f\"{model_key}\"]  = model_.decision_function(X_test_external)\n",
    "            \n",
    "except Exception as e:\n",
    "Print (f \"Error processing model {model_key}: {str (e)}\")\n",
    "continue\n",
    "##ROC curve\n",
    "###External ROC\n",
    "#Create a graphic object\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "\n",
    "for model_key, model_name in models.items():\n",
    "   \n",
    "    prediction = predictions_external[model_key]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_external, prediction)\n",
    "    roc_auc_test = auc(fpr, tpr)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    \n",
    "    plt.plot(fpr, tpr, lw=1, label=f'{model_key} (area = {roc_auc:.4f})')\n",
    "    \n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(\"ROC Curves Comparison (External Validation Set)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "#Traverse each model in order\n",
    "for model_key, model_name in models.items():\n",
    "#Obtain the prediction results of the current model\n",
    "prediction = predictions[model_key]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, prediction)\n",
    "roc_auc_test = auc(fpr, tpr)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "#Draw the ROC curve of the current model\n",
    "plt.plot(fpr, tpr, lw=1, label=f'{model_key} (area = {roc_auc:.4f})')\n",
    "    \n",
    "#Draw diagonal lines as reference\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "\n",
    "#Set graphic properties\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(\"ROC Curves Comparison (Internal Test Set)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.savefig('D:/JupyterLab/AKI/Result/Fig/test_set_roc_curves5.27.pdf', \n",
    "format='pdf', dpi=800, bbox_inches='tight')\n",
    "## DCA\n",
    "# 1. Net benefits brought by computational models\n",
    "def calculate_net_benefit_model(thresh_group, y_pred_score, y_label):\n",
    "    net_benefit_model = np.array([])\n",
    "    for thresh in thresh_group:\n",
    "        y_pred_label = y_pred_score > thresh\n",
    "        tn, fp, fn, tp = confusion_matrix(y_label, y_pred_label).ravel()\n",
    "        n = len(y_label)\n",
    "        net_benefit = (tp / n) - (fp / n) * (thresh / (1 - thresh))\n",
    "        net_benefit_model = np.append(net_benefit_model, net_benefit)\n",
    "    return net_benefit_model\n",
    "# 2. Calculate the net benefits brought by the treat all strategy\n",
    "def calculate_net_benefit_all(thresh_group, y_label):\n",
    "net_benefit_all = np.array([])\n",
    "tn, fp, fn, tp = confusion_matrix(y_label, y_label).ravel()\n",
    "total = tp + tn\n",
    "for thresh in thresh_group:\n",
    "net_benefit = (tp / total) - (tn / total) * (thresh / (1 - thresh))\n",
    "net_benefit_all = np.append(net_benefit_all, net_benefit)\n",
    "return net_benefit_all\n",
    "#Construct a model with poor classification performance\n",
    "thresh_group = np.arange(0,1,0.001)\n",
    "###Internal data\n",
    "predictions['Bagging']\n",
    "#Internal data\n",
    "prediction = predictions[model_key]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, prediction)\n",
    "net_benefit_model_XGBoost = calculate_net_benefit_model(thresh_group, predictions['XGBoost'], y_test)\n",
    "net_benefit_model_ANN = calculate_net_benefit_model(thresh_group, predictions['ANN'], y_test)\n",
    "net_benefit_model_KNN = calculate_net_benefit_model(thresh_group, predictions['KNN'], y_test)\n",
    "net_benefit_model_Bagging = calculate_net_benefit_model(thresh_group, predictions['Bagging'], y_test)\n",
    "net_benefit_model_AdaBoost = calculate_net_benefit_model(thresh_group, predictions['AdaBoost'], y_test)\n",
    "net_benefit_model_Random_Forest = calculate_net_benefit_model(thresh_group, predictions['Random Forest'], y_test)\n",
    "net_benefit_model_Logistic = calculate_net_benefit_model(thresh_group, predictions['Logistic'], y_test)\n",
    "net_benefit_model_Catboost = calculate_net_benefit_model(thresh_group, predictions['Catboost'], y_test)\n",
    "net_benefit_all = calculate_net_benefit_all(thresh_group, y_test)\n",
    "###Drawing\n",
    "#Internal Data DCA Curve\n",
    "Fig, ax=plt. subplots (figsize=(10, 6)) # Adjust chart size\n",
    "\n",
    "#Define a more discriminative color scheme\n",
    "colors = [\n",
    "'#FF7F50', #Coral Red\n",
    "'#6A5ACD', #Stone slab blue\n",
    "'#00CED1', #Dark Blue\n",
    "'#FFA500', #Orange\n",
    "'#CD5C5C', #Indian Red\n",
    "'#20B2AA', #Bright Steel Blue\n",
    "'#9370DB', #Dark Violet\n",
    "'#3CB371',  # MediumSeaGreen\n",
    "]\n",
    "\n",
    "#Draw model curve\n",
    "models = [\n",
    "(net_benefit_model_ANN, 'ANN'),\n",
    "(net_benefit_model_KNN, 'KNN'),\n",
    "(net_benefit_model_XGBoost, 'XGBoost'),\n",
    "(net_benefit_model_AdaBoost, 'AdaBoost'),\n",
    "(net_benefit_model_Catboost, 'Catboost'),\n",
    "(net_benefit_model_Random_Forest, 'Random Forest'),\n",
    "(net_benefit_model_Bagging, 'Bagging'),\n",
    "(net_benefit_model_Logistic, 'Logistic')\n",
    "]\n",
    "\n",
    "#Draw model curves using loops\n",
    "for i, (model, label) in enumerate(models):\n",
    "Ax. plot (thresh_group, model, color=colors [i], label=label, lw=1.5) # Increase line width\n",
    "\n",
    "#Draw reference lines\n",
    "ax.plot(thresh_group, net_benefit_all, color='#808080', label='Treat all', lw=1.5)  #  dark gray\n",
    "ax.plot((0, 1), (0, 0), color='#808080', linestyle=':', label='Treat none', lw=1.5)\n",
    "\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(-0.1, 0.6)\n",
    "ax.set_xlabel(\n",
    "    xlabel='Threshold Probability',\n",
    "    fontdict={'family': 'Times New Roman', 'fontsize': 15}\n",
    ")\n",
    "ax.set_ylabel(\n",
    "    ylabel='Net Benefit',\n",
    "    fontdict={'family': 'Times New Roman', 'fontsize': 15}\n",
    ")\n",
    "\n",
    "\n",
    "ax.spines['right'].set_color((0.8, 0.8, 0.8))\n",
    "ax.spines['top'].set_color((0.8, 0.8, 0.8))\n",
    "\n",
    "legend = ax.legend(loc='upper right', frameon=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('white')\n",
    "frame.set_edgecolor('#e0e0e0')\n",
    "\n",
    "plt.tight_layout()  \n",
    "plt.savefig('D:/JupyterLab/AKI/Result/Fig/DCA_internal5.27.pdf', \n",
    "            format='pdf', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "###External data\n",
    "#Construct a model with poor classification performance\n",
    "thresh_group = np.arange(0,1,0.001)\n",
    "#External data\n",
    "prediction_external = predictions_external[model_key]\n",
    "fpr, tpr, thresholds = roc_curve(y_test_external, prediction_external)\n",
    "net_benefit_model_external_XGBoost= calculate_net_benefit_model(thresh_group, predictions_external['XGBoost'], y_test_external)\n",
    "net_benefit_model_external_ANN= calculate_net_benefit_model(thresh_group, predictions_external['ANN'], y_test_external)\n",
    "net_benefit_model_external_KNN= calculate_net_benefit_model(thresh_group, predictions_external['KNN'], y_test_external)\n",
    "net_benefit_model_external_Bagging= calculate_net_benefit_model(thresh_group, predictions_external['Bagging'], y_test_external)\n",
    "net_benefit_model_external_AdaBoost= calculate_net_benefit_model(thresh_group, predictions_external['AdaBoost'], y_test_external)\n",
    "net_benefit_model_external_Random_Forest= calculate_net_benefit_model(thresh_group, predictions_external['Random Forest'], y_test_external)\n",
    "net_benefit_model_external_Logistic= calculate_net_benefit_model(thresh_group, predictions_external['Logistic'], y_test_external)\n",
    "net_benefit_model_external_Catboost= calculate_net_benefit_model(thresh_group, predictions_external['Catboost'], y_test_external)\n",
    "#External data\n",
    "net_benefit_all_external = calculate_net_benefit_all(thresh_group, y_test_external)\n",
    "###Drawing\n",
    "#DCA curve of external data\n",
    "fig, ax = plt.subplots(figsize=(10, 6)) \n",
    "\n",
    "colors = [\n",
    "    '#FF7F50',  \n",
    "    '#6A5ACD',  \n",
    "    '#00CED1',  \n",
    "    '#FFA500',  \n",
    "    '#CD5C5C',  \n",
    "    '#20B2AA',  \n",
    "    '#9370DB',  \n",
    "    '#3CB371', \n",
    "]\n",
    "\n",
    "\n",
    "models = [\n",
    "    (net_benefit_model_external_ANN, 'ANN'),\n",
    "    (net_benefit_model_external_KNN, 'KNN'),\n",
    "    (net_benefit_model_external_XGBoost, 'XGBoost'),\n",
    "    (net_benefit_model_external_AdaBoost, 'AdaBoost'),\n",
    "    (net_benefit_model_external_Catboost, 'Catboost'),\n",
    "    (net_benefit_model_external_Random_Forest, 'Random Forest'),\n",
    "    (net_benefit_model_external_Bagging, 'Bagging'),\n",
    "    (net_benefit_model_external_Logistic, 'Logistic')\n",
    "]\n",
    "\n",
    "\n",
    "for i, (model, label) in enumerate(models):\n",
    "    ax.plot(thresh_group, model, color=colors[i], label=label, lw=1.5, alpha=0.85) \n",
    "\n",
    "ax.plot(thresh_group, net_benefit_all_external, color='#808080', label='Treat all', lw=1.5)  \n",
    "ax.plot((0, 1), (0, 0), color='#808080', linestyle=':', label='Treat none', lw=1.5)\n",
    "\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(-0.1, 0.4)  \n",
    "ax.set_xlabel(\n",
    "    xlabel='Threshold Probability',\n",
    "    fontdict={'family': 'Times New Roman', 'fontsize': 15}\n",
    ")\n",
    "ax.set_ylabel(\n",
    "    ylabel='Net Benefit',\n",
    "    fontdict={'family': 'Times New Roman', 'fontsize': 15}\n",
    ")\n",
    "\n",
    "\n",
    "ax.spines['right'].set_color((0.8, 0.8, 0.8))\n",
    "ax.spines['top'].set_color((0.8, 0.8, 0.8))\n",
    "ax.spines['left'].set_linewidth(1.2)  \n",
    "ax.spines['bottom'].set_linewidth(1.2)  \n",
    "\n",
    "\n",
    "legend = ax.legend(loc='upper right', frameon=True, ncol=2)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('white')\n",
    "frame.set_edgecolor('#e0e0e0')\n",
    "frame.set_alpha(0.95)  \n",
    "\n",
    "ax.grid(True, axis='y', linestyle='--', alpha=0.3)\n",
    "\n",
    "\n",
    "plt.tight_layout()  \n",
    "#plt.savefig('D:/JupyterLab/AKI/Result/Fig/DCA_external5.27.pdf', \n",
    "#           format='pdf', dpi=800, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "\n",
    "selected_models = {\n",
    "    \"AdaBoost\": ADB_model,\n",
    "    \"Bagging\": Bagging_model,\n",
    "    \"ANN\": ANN_model,\n",
    "    \"Random Forest\": RF_model,\n",
    "    \"KNN\": KNN_model,\n",
    "    \"CatBoost\": Catboost_model,\n",
    "    \"Logistic\": Logistic_model,\n",
    "    \"XGBoost\": XGB_model\n",
    "}\n",
    "\n",
    "predictions_internal_selected = {}\n",
    "predictions_external_selected = {}\n",
    "\n",
    "for model_key, model in selected_models.items():\n",
    "    print(f\"Processing model: {model_key}\")\n",
    "    try:\n",
    "       #Internal data prediction\n",
    "if hasattr(model, \"predict_proba\"):\n",
    "predictions_internal_selected[model_key] = model.predict_proba(X_test)[:, 1]\n",
    "else:\n",
    "predictions_internal_selected[model_key] = model.decision_function(X_test)\n",
    "\n",
    "#External data prediction\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            predictions_external_selected[model_key] = model.predict_proba(X_test_external)[:, 1]\n",
    "        else:\n",
    "            predictions_external_selected[model_key] = model.decision_function(X_test_external)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while processing model {model_key}: {e}\")\n",
    "        continue\n",
    "\n",
    "def safe_normalize(pred):\n",
    "    pred = np.array(pred)\n",
    "    if pred.min() == pred.max():  \n",
    "        return np.clip(pred, 0, 1)\n",
    "    return (pred - pred.min()) / (pred.max() - pred.min())\n",
    "\n",
    "print(\"Internal Data Brier Scores:\")\n",
    "for model_key, score in brier_scores_internal.items():\n",
    "    print(f\"{model_key}: {score:.4f}\")\n",
    "\n",
    "### Brier Score\n",
    "\n",
    "print(\"External Data Brier Scores:\")\n",
    "for model_key, score in brier_scores_external.items():\n",
    "    print(f\"{model_key}: {score:.4f}\")\n",
    "## Variable Importance\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cm\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "def variableImportance(model, X, y):\n",
    "    result = permutation_importance(\n",
    "        model, X, y, n_repeats=10, random_state=42, n_jobs=4\n",
    "    )\n",
    "    return result.importances_mean\n",
    "\n",
    "imp = pd.DataFrame(\n",
    "    {\n",
    "        model_name: variableImportance(model, X_test, y_test)\n",
    "        for model_name, model in selected_models.items()\n",
    "    },\n",
    "    index=column_name\n",
    ")\n",
    "imp['Average'] = imp.mean(axis=1)\n",
    "\n",
    "def plotFeatureImportance(a):\n",
    "    mpl.rcParams['figure.dpi'] = 300\n",
    "    a = a.sort_values(by=['Average'], ascending=False)\n",
    "    fig, ax = plt.subplots(figsize=(8, 10))\n",
    "\n",
    "    def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "        new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "            'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "            cmap(np.linspace(minval, maxval, n)))\n",
    "        return new_cmap\n",
    "\n",
    "    im = ax.imshow(a[list(selected_models.keys()) + ['Average']],\n",
    "                   cmap=truncate_colormap(cm.get_cmap(\"hot\"), minval=0.4, maxval=1), norm=colors.SymLogNorm(1e-3))\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    cbar.ax.set_ylabel(\"Permutation Feature Importance\", rotation=-90, va=\"bottom\")\n",
    "    cbar.set_ticks([0.1, 0.01, 0.001, 0, -0.001])\n",
    "    cbar.set_ticklabels(['10', \"1\", \"0.1\", 0, '-1'])\n",
    "\n",
    "    ax.set_yticks(np.arange(len(a)))\n",
    "    ax.set_xticks(np.arange(len(a.columns)))\n",
    "    ax.set_xticklabels(list(selected_models.keys()) + ['Average'])\n",
    "    ax.set_yticklabels(a.index)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "plotFeatureImportance(imp)\n",
    "plt.savefig('D:/JupyterLab/AKI/Result/Fig/feature_importance5.27.pdf', \n",
    "           format='pdf', dpi=300, bbox_inches='tight')\n",
    "## Shap\n",
    "import shap\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "### XGBOOST\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def XGBoost_prediction(X):\n",
    "    return XGB_model.predict(X)\n",
    "\n",
    "#XGBoost_explainer = shap.TreeExplainer(XGBoost_prediction, X_train)\n",
    "XGBoost_explainer = shap.TreeExplainer(XGB_model)\n",
    "XGBoost_shap_values = XGBoost_explainer.shap_values(X_train)\n",
    "\n",
    "XGBoost_shap_values = XGBoost_explainer(X_train)\n",
    "\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 0.5 * num_features))\n",
    "shap.summary_plot(\n",
    "    XGBoost_shap_values.values,  \n",
    "    X_train,\n",
    "    max_display=num_features,\n",
    "    show=False\n",
    ")\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_xlim(-0.6, 0.6)  \n",
    "ticks = np.arange(-0.6, 0.6 + 0.2, 0.2) \n",
    "ax.set_xticks(ticks)\n",
    "ax.set_xticklabels([f\"{tick:.1f}\" for tick in ticks], rotation=0, fontsize=8)\n",
    "\n",
    "ax.grid(False)  \n",
    "ax.yaxis.grid(False) \n",
    "\n",
    "plt.savefig(\n",
    "    \"D:/JupyterLab/AKI/Result/Fig/XGBoost_shap_summary_all_features610.pdf\",\n",
    "    format=\"pdf\",\n",
    "    dpi=800,\n",
    "    bbox_inches=\"tight\"\n",
    ")\n",
    "plt.close()\n",
    "plt.show()\n",
    "\n",
    "shap.dependence_plot(\n",
    "    \"AIP\",                           \n",
    "    XGBoost_shap_values.values,       \n",
    "    X_train,                         \n",
    "    interaction_index=None,          \n",
    "    show=False                        \n",
    ")\n",
    "\n",
    "\n",
    "plt.title(\"SHAP Dependence Plot for AIP\", fontsize=12)\n",
    "plt.xlabel(\"AIP Value\", fontsize=10)\n",
    "plt.ylabel(\"SHAP Value (Impact on Prediction)\", fontsize=10)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, cohen_kappa_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#Define the function for calculating indicators\n",
    "def calculate_metrics(y_true, y_pred_probs):\n",
    "#Generate category predictions based on predicted probabilities\n",
    "If np. min (y_pred_debs)<0: # Decision function values may have negative numbers\n",
    "y_pred = (y_pred_probs >= 0).astype(int)\n",
    "Else: # Probability values between 0-1\n",
    "y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "    \n",
    "metrics = {}\n",
    "    \n",
    "#Basic indicators\n",
    "Metrics [Error Rate]=1- Accuracy_score (y_true, y_pred)\n",
    "Metrics [accuracy]=accuracy_store (y_true, y_pred)\n",
    "Metrics [precision]=precision_store (y_true, y_pred)\n",
    "Metrics [recall rate]=recall_stcore (y_true, y_pred)\n",
    "Metrics ['F1 value ']=f1_score (y_true, y_pred)\n",
    "    \n",
    "# AUC\n",
    "Metrics ['AUC value ']=roc_ auc_score (y_true, y_pred_debs)\n",
    "    \n",
    "#Cohen Kappa\n",
    "Metrics [Cohen score]=Cohen, kappa, score (y_true, y_pred)\n",
    "    \n",
    "#Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "Metrics [confusion matrix]=f \"TN={cm [0,0]}, FP={cm [0,1]}, FN={cm [1,0]}, TP={cm [1,1]}\"\n",
    "    \n",
    "return metrics\n",
    "\n",
    "#Initialization result storage\n",
    "internal_results = {}\n",
    "external_results = {}\n",
    "\n",
    "#Handling internal test sets\n",
    "for model_key, preds in predictions.items():\n",
    "try:\n",
    "#Calculate indicators\n",
    "internal_results[model_key] = calculate_metrics(\n",
    "y_test, \n",
    "preds\n",
    ")\n",
    "except Exception as e:\n",
    "Print (f \"Error calculating internal metrics of model {model_key}: {str (e)}\")\n",
    "Internal_desults [model_key]={Error: str (e)}\n",
    "\n",
    "#Processing external validation sets\n",
    "for model_key, preds in predictions_external.items():\n",
    "try:\n",
    "#Calculate indicators\n",
    "external_results[model_key] = calculate_metrics(\n",
    "y_test_external, \n",
    "preds\n",
    ")\n",
    "except Exception as e:\n",
    "Print (f \"Error calculating external metrics for model {model_key}: {str (e)}\")\n",
    "External_ results [model_key]={Error: str (e)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8819c8ec",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def plot_idi(y_truth: np.ndarray, ref_model: np.ndarray, new_model: np.ndarray,\n",
    "             thresholds: Dict[float, str],num_bootstraps: int = 1000, show: bool = True, save: bool = False) ->  Dict:\n",
    "    \"\"\"\n",
    "    Plot the Integrated Discrimination Improvement (IDI) curve.\n",
    "\n",
    "    Args:\n",
    "        y_truth (np.ndarray): Ground truth labels.\n",
    "        ref_model (np.ndarray): Reference model predictions.\n",
    "        new_model (np.ndarray): New model predictions.\n",
    "        thresholds (dict): Dictionary of thresholds with their corresponding labels.\n",
    "        num_bootstraps (int, optional): Number of bootstrap iterations. Default is 1000.\n",
    "        save (bool, optional): Whether to save the plot. Default is False.\n",
    "    Returns:\n",
    "        Dict: A dictionary containing the plot, IDI statistics, and NRI values.\n",
    "            - 'plot': The matplotlib figure object of the IDI curve.\n",
    "            - 'IDI': A dictionary of IDI statistics including overall IDI, IDI for events, and IDI for nonevents.\n",
    "            - 'NRI': A dictionary of NRI values at specified thresholds.\n",
    "    Note:\n",
    "        - This function requires the `matplotlib`, `numpy`, and `sklearn` packages.\n",
    "        - The thresholds dictionary should contain values between 0 and 100, representing percentages.\n",
    "    \"\"\"\n",
    "    \n",
    "    ref_fpr, ref_tpr, ref_thresholds = metrics.roc_curve(y_truth, ref_model)\n",
    "    new_fpr, new_tpr, new_thresholds = metrics.roc_curve(y_truth, new_model)\n",
    "    base, mean_tprs, mean_fprs,_,_ = bootstrap_results(y_truth, new_model, num_bootstraps)\n",
    "    base2, mean_tprs2, mean_fprs2,_,_ = bootstrap_results(y_truth, ref_model, num_bootstraps)\n",
    "    is_pos, is_neg, idi_event = area_between_curves(mean_tprs, mean_tprs2)\n",
    "    ip_pos, ip_neg, idi_nonevent = area_between_curves(mean_fprs2, mean_fprs)\n",
    "\n",
    "    plt.ioff()\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    #ax = ax.axes()\n",
    "    lw = 2\n",
    "    ax.plot(base, mean_tprs, 'steelblue', alpha=0.5, label='Events New (New)')\n",
    "    ax.plot(base, mean_fprs, 'deeppink', alpha=0.5, label='Nonevents New (New)')\n",
    "    ax.plot(base2, mean_tprs2, 'steelblue', alpha=0.7, linestyle='--', label='Events Reference (Ref)')\n",
    "    ax.plot(base2, mean_fprs2, 'deeppink', alpha=0.7, linestyle='--', label='Nonevents Reference (Ref)')\n",
    "    ax.fill_between(base, mean_tprs, mean_tprs2, color='steelblue', alpha=0.2,\n",
    "                     label='Integrated Sensitivity (area = %0.2f)' % idi_event)\n",
    "    ax.fill_between(base, mean_fprs, mean_fprs2, color='deeppink', alpha=0.2,\n",
    "                     label='Integrated Specificity (area = %0.2f)' % idi_nonevent)\n",
    "\n",
    "    def nri_annotation(plt, threshold, label, color):\n",
    "        \"\"\"\n",
    "        Annotate the plot with NRI information for a given threshold.\n",
    "        \n",
    "        This function adds vertical lines, text annotations, and arrow annotations to the plot\n",
    "        to represent the Net Reclassification Improvement (NRI) for events and nonevents at a specific threshold.\n",
    "        The function modifies the provided matplotlib plot in place.\n",
    "        \n",
    "        Args:\n",
    "        plt (matplotlib.pyplot): The matplotlib plotting module or a matplotlib Axes object.\n",
    "        threshold (int or float): The threshold value at which to calculate and annotate the NRI.\n",
    "                                  This should correspond to a point on the ROC curve.\n",
    "        label (str): The label to use for the vertical line annotation at the threshold.\n",
    "        color (str or tuple): The color to use for the annotations. This can be a named color,\n",
    "                              a hex color code, or an RGB tuple.\n",
    "        \n",
    "        Returns:\n",
    "        tuple: A tuple containing the threshold, NRI for events, NRI for nonevents, and total NRI.\n",
    "               Formatted as (threshold, NRI_events, NRI_nonevents, total_NRI).\n",
    "        \n",
    "        Note:\n",
    "        - This function is intended to be used as part of the `plot_idi` function.\n",
    "        - The `threshold` argument is expected to be within the range of the model's prediction scores.\n",
    "        \"\"\"\n",
    "        x_pos = base[threshold]\n",
    "        x_offset = 0.02\n",
    "        x_offset2 = x_offset\n",
    "        text_y_offset = 0.01\n",
    "        text_y_offset2 = text_y_offset\n",
    "        if threshold == 2:\n",
    "            text_y_offset = 0.04\n",
    "            text_y_offset2 = 0.04\n",
    "            x_offset2 = 0.05\n",
    "            \n",
    "        text_y_events = np.mean([mean_tprs2[threshold], mean_tprs[threshold]]) + text_y_offset\n",
    "        text_y_nonevents = np.mean([mean_fprs[threshold], mean_fprs2[threshold]]) + text_y_offset2\n",
    "\n",
    "        ax.axvline(x=threshold/100, color=color, linestyle='--', alpha=0.5, label=label)\n",
    "        ax.annotate('', xy=(x_pos + 0.02, mean_tprs2[threshold + 1]), xycoords='data',\n",
    "                     xytext=(x_pos + 0.02, mean_tprs[threshold]), textcoords='data', arrowprops={'arrowstyle': '|-|'})\n",
    "        ax.annotate('NRI$_{events}$ = %0.2f' % (mean_tprs[threshold] - mean_tprs2[threshold]),\n",
    "                     xy=(x_pos + x_offset, text_y_events), xycoords='data',\n",
    "                     xytext=(x_pos + x_offset, text_y_events),\n",
    "                     textcoords='offset points', fontsize=10)\n",
    "        ax.annotate('', xy=(x_pos + 0.02, mean_fprs[threshold]), xycoords='data',\n",
    "                     xytext=(x_pos + 0.02, mean_fprs2[threshold]), textcoords='data',\n",
    "                     arrowprops=dict(arrowstyle='|-|', color='r'))\n",
    "        ax.annotate('NRI$_{nonevents}$ = %0.2f' % (mean_fprs2[threshold] - mean_fprs[threshold]),\n",
    "                     xy=(x_pos + x_offset2, text_y_nonevents), xycoords='data',\n",
    "                     xytext=(x_pos + x_offset2, text_y_nonevents),\n",
    "                     textcoords='offset points', fontsize=10)\n",
    "        return x_pos, mean_tprs[threshold] - mean_tprs2[threshold], mean_fprs2[threshold] - mean_fprs[threshold], (mean_tprs[threshold] - mean_tprs2[threshold]) + (mean_fprs2[threshold] - mean_fprs[threshold])\n",
    "        \n",
    "    def generate_distinct_colors(list_size: int, avoid_red: bool = True, avoid_dark: bool = True) -> list:\n",
    "        \"\"\"\n",
    "        Generate a list of distinct colors, optionally avoiding red and dark (near black) colors.\n",
    "    \n",
    "        Args:\n",
    "            list_size (int): The number of distinct colors to generate.\n",
    "            avoid_red (bool): Whether to avoid red colors. Defaults to True.\n",
    "            avoid_dark (bool): Whether to avoid dark colors. Defaults to True.\n",
    "    \n",
    "        Returns:\n",
    "            list: A list of colors in hexadecimal format.\n",
    "        \"\"\"\n",
    "        colors = []\n",
    "        # Define the HSV range to avoid red and dark colors\n",
    "        hue_start = 0.1 if avoid_red else 0\n",
    "        hue_end = 0.9 if avoid_red else 1\n",
    "        saturation = 0.7\n",
    "        value = 0.9 if avoid_dark else 0.5\n",
    "        # Use HSV tuples to evenly space out colors and convert to RGB\n",
    "        for i in np.linspace(hue_start, hue_end, list_size, endpoint=False):\n",
    "            rgb_color = mcolors.hsv_to_rgb((i, saturation, value))  # Saturation and Value adjusted for color brightness\n",
    "            hex_color = mcolors.rgb2hex(rgb_color)\n",
    "            colors.append(hex_color)\n",
    "        \n",
    "        return colors\n",
    "        \n",
    "    colors = generate_distinct_colors(len(thresholds))\n",
    "    nris = {}\n",
    "    for c, i in enumerate(thresholds):\n",
    "        thresh, nri_events, nri_nonevents, total_nri = nri_annotation(plt, int(i), thresholds[i], colors[c])\n",
    "        nris[thresh] = [nri_events, nri_nonevents, total_nri]\n",
    "        \n",
    "    ax.set_xlim([0.0, 1.10])\n",
    "    ax.set_ylim([0.0, 1.10])\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.set_xlabel('Calculated Risk', fontsize=18)\n",
    "    ax.set_ylabel('Sensitivity (black), 1 - Specificity (red)', fontsize=18)\n",
    "    ax.legend(loc=\"upper right\", fontsize=11)\n",
    "    ax.legend(loc=0, fontsize=11, bbox_to_anchor=(0, 0, 1.2, .9))\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    if save:\n",
    "        fig.savefig('idi_curve.png', dpi=300, bbox_inches='tight')\n",
    "    if show:\n",
    "        print('IS positive', round(is_pos, 2), 'IS negative', round(is_neg, 2), 'IDI events', round(idi_event, 2))\n",
    "        print('IP positive', round(ip_pos, 2), 'IP negative', round(ip_neg, 2), 'IDI nonevents', round(idi_nonevent, 2))\n",
    "        print('IDI =', round(idi_event + idi_nonevent, 2))\n",
    "        fig.show()\n",
    "        \n",
    "    idi_stats = {'IDI':idi_event + idi_nonevent, 'IDI Events':idi_event, 'IDI Nonevents':idi_nonevent, 'IS Positive':is_pos, 'IS Negative':is_neg, 'IP Positive':ip_pos, 'IP Negative':ip_neg}\n",
    "    outputs = {'plot':fig,'IDI':idi_stats,'NRI':nris}\n",
    "    return outputs #idi_event + idi_nonevent,idi_event, idi_nonevent, is_pos, is_neg, ip_pos, ip_neg, fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
